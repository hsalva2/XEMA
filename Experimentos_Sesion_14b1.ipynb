{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsalva2/XEMA/blob/main/Experimentos_Sesion_14b1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Double Machine Learning & Generalized Random Forests"
      ],
      "metadata": {
        "id": "QMHvQEVbgglZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install econml"
      ],
      "metadata": {
        "id": "B_3ktMhMQrrF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cM0wOkmvtYAi"
      },
      "outputs": [],
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from econml.dml import LinearDML, SparseLinearDML, CausalForestDML, NonParamDML\n",
        "from econml.grf import CausalForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMY0XeqatgfD"
      },
      "outputs": [],
      "source": [
        "# Load the experiment dataset\n",
        "datos = pd.read_csv('https://raw.githubusercontent.com/carlosquintanillaa/Datasets/refs/heads/main/df1.csv')\n",
        "nuevos = pd.read_csv('https://raw.githubusercontent.com/carlosquintanillaa/Datasets/refs/heads/main/df2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWKReImjziRf"
      },
      "outputs": [],
      "source": [
        "# Definicion de variables para datos de los que aprenderemos los modelos\n",
        "y = datos['Y']\n",
        "T = datos['T']\n",
        "X = datos.drop(['id','Y','T'],axis=1)\n",
        "X = pd.get_dummies(X,drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnzwSouuz19B"
      },
      "outputs": [],
      "source": [
        "# Definir roles para datos nuevos\n",
        "X2 = nuevos.drop(['id'],axis=1)\n",
        "X2 = pd.get_dummies(X2,drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelos para g(X,W) y m(X,W)\n",
        "model_g = LinearRegression()\n",
        "model_m = DummyClassifier(strategy='prior')\n",
        "model_f = GradientBoostingRegressor()"
      ],
      "metadata": {
        "id": "UqmzWxxamanl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOjjn2ys0R64"
      },
      "outputs": [],
      "source": [
        "# Modelo 01 : Linear DML\n",
        "est1 = LinearDML(model_y=model_g, model_t=model_m,discrete_treatment=True)\n",
        "est1.fit(y,T,X=X)\n",
        "efecto1 = est1.effect(X2)\n",
        "# CORREGI EL CRITERIO DE DECISION A EFFECTO1 > 0.75\n",
        "decision1 = np.where(efecto1 > 0.75,1,0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "est1.summary()"
      ],
      "metadata": {
        "id": "pqMLuqJYWU4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vd1tBTtA0w2m"
      },
      "outputs": [],
      "source": [
        "# Modelo 02 : Sparse Linear DML\n",
        "est2 = SparseLinearDML(model_y=model_g, model_t=model_m,discrete_treatment=True)\n",
        "est2.fit(y,T,X=X)\n",
        "efecto2 = est2.effect(X2)\n",
        "# CORREGI EL CRITERIO DE DECISION A EFFECTO2 > 0.75\n",
        "decision2 = np.where(efecto2 > 0.75,1,0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "est2.summary()"
      ],
      "metadata": {
        "id": "_8bNC0IYWY7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35M4WwKX1sJc"
      },
      "outputs": [],
      "source": [
        "# Modelo 03 : CausalForestDML\n",
        "est3 = CausalForestDML(model_y=model_g, model_t=model_m,discrete_treatment=True)\n",
        "est3.fit(y,T,X=X)\n",
        "efecto3 = est3.effect(X2)\n",
        "# CORREGI EL CRITERIO DE DECISION A EFFECTO3 > 0.75\n",
        "decision3 = np.where(efecto3 > 0.75,1,0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CausalForestDML tiene un output muy util: Cual es la importancia de las varaibles en la funcion final de CATE\n",
        "feature_importances3 = est3.feature_importances_\n",
        "feature_importances3"
      ],
      "metadata": {
        "id": "P0IkQ-m4nnqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hagamoslo mas interpretable. Hagamos un DataFrame\n",
        "feature_names = X2.columns.tolist()\n",
        "feature_importances3 = est3.feature_importances_\n",
        "\n",
        "# Convert to a DataFrame\n",
        "feature_importances3_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': feature_importances3\n",
        "})\n",
        "\n",
        "feature_importances3_df"
      ],
      "metadata": {
        "id": "ftkrlDiQn1j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora grafiquemoslo\n",
        "feature_importances3_df = feature_importances3_df.sort_values('Importance', ascending=False)\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importances3_df)"
      ],
      "metadata": {
        "id": "9ktSDi9Pn7PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gy8ozEPO2CAA"
      },
      "outputs": [],
      "source": [
        "# Modelo 04 : NonParamDML (GradientBoostingRegressor)\n",
        "est4 = NonParamDML(model_final=model_f, model_y=model_g, model_t=model_m,discrete_treatment=True)\n",
        "est4.fit(y,T,X=X)\n",
        "efecto4 = est4.effect(X2)\n",
        "# CORREGI EL CRITERIO DE DECISION A EFFECTO4 > 0.75\n",
        "decision4 = np.where(efecto4 > 0.75,1,0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelo 05 : Generalized Random Forest\n",
        "est5 = CausalForest(random_state=1234)\n",
        "est5.fit(X,T,y)\n",
        "efecto5 = est5.predict(X2).ravel()\n",
        "decision5 = np.where(efecto4 > 0.75,1,0)"
      ],
      "metadata": {
        "id": "TJhkybtg8iiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CausalForestDML tiene un output muy util: Cual es la importancia de las varaibles en la funcion final de CATE\n",
        "feature_importances5 = est5.feature_importances_\n",
        "feature_importances5"
      ],
      "metadata": {
        "id": "8dP6ZttYNDTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hagamoslo mas interpretable. Hagamos un DataFrame\n",
        "feature_names = X2.columns.tolist()\n",
        "feature_importances5 = est5.feature_importances_\n",
        "\n",
        "# Convert to a DataFrame\n",
        "feature_importances5_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': feature_importances5\n",
        "})\n",
        "\n",
        "feature_importances5_df"
      ],
      "metadata": {
        "id": "OpEwvWYuNVaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora grafiquemoslo\n",
        "feature_importances5_df = feature_importances5_df.sort_values('Importance', ascending=False)\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importances5_df)"
      ],
      "metadata": {
        "id": "mWOxA8pZNQsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNMNaMG_5-Ix"
      },
      "outputs": [],
      "source": [
        "# Guardar Efectos y Decisiones\n",
        "efectos = pd.DataFrame({'efecto1':efecto1,'efecto2':efecto2,'efecto3':efecto3,'efecto4':efecto4,'efecto5':efecto5})\n",
        "decisiones = pd.DataFrame({'decision1':decision1,'decision2':decision2,'decision3':decision3,'decision4':decision4,'decision5':decision5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5nZzv9QQmix"
      },
      "outputs": [],
      "source": [
        "# Cuales son los efectos heterogeneos de acuerdo a los distintos modelos?\n",
        "# Prueben efecto1, efecto2, efecto3, efecto4. Cual aproxima mejor la verdadera funcion CATE = -2.5 + 5*Abs(X4)\n",
        "sns.relplot(x=nuevos['X4'],y=efectos['efecto1'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "causality",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}